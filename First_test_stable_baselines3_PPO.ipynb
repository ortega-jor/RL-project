{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from tqdm import tqdm  # Import tqdm for progress bar (I havent been able to implement it using this stable baseline libraries)\n",
    "\n",
    "import torch # Might be useful in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure MuJoCo is using the correct OpenGL backend\n",
    "os.environ[\"MUJOCO_GL\"] = \"glfw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create and visualize the environment\n",
    "def modify_mass_parameters(model):\n",
    "    # Modify the mass of specific body parts\n",
    "    model.body_mass[1] = 4.15  # Set mass of torso \n",
    "    model.body_mass[2], model.body_mass[5] = 0.6, 0.6  # Set mass of thigh\n",
    "    model.body_mass[3], model.body_mass[6] = 0.3, 0.3  # Set mass of leg\n",
    "    model.body_mass[4], model.body_mass[7]= 0.1, 0.1  # Set mass of foot\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"Walker2d-v5\", render_mode=None)  # No GUI during training\n",
    "    # Access the MuJoCo model and modify it\n",
    "    model = env.unwrapped.model\n",
    "    # modify_mass_parameters(model) # Modify mass parameters for the environment's model\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use multiple environments for faster training\n",
    "num_envs = 4  # Increase this for better training speed (dont put more than your computer cores)\n",
    "env = SubprocVecEnv([make_env for _ in range(num_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define PPO model with tuned hyperparameters\n",
    "ppo_model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    "    device=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2143 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1286        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018404648 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.48       |\n",
      "|    explained_variance   | -0.122      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.47        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 7.24        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1190        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019686723 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.45       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.85        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1127        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016646927 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.41       |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.53        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1095        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012470888 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.37       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1061        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010625402 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.34       |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 57.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009587303 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.31       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1032        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009206582 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1030         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096061425 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.23        |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 63.4         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    std                  | 0.952        |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1030        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009642912 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1023        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008517436 |\n",
      "|    clip_fraction        | 0.0889      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 179         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1022        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009744419 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.3        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1014         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098521765 |\n",
      "|    clip_fraction        | 0.0986       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.18        |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 78.4         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0156      |\n",
      "|    std                  | 0.945        |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1da3a4251f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the model\n",
    "# This is going to print something like this: Look at \"total_timesteps\" to see how long of a training is left\n",
    "\"\"\"\n",
    "    ----------------------------------------\n",
    "    | time/                   |            |\n",
    "    |    fps                  | 943        |\n",
    "    |    iterations           | 8          |\n",
    "    |    time_elapsed         | 69         |\n",
    "    |    total_timesteps      | 65536      |\n",
    "    | train/                  |            |\n",
    "    |    approx_kl            | 0.00866781 |\n",
    "    |    clip_fraction        | 0.0932     |\n",
    "    |    clip_range           | 0.2        |\n",
    "    |    entropy_loss         | -8.32      |\n",
    "    |    explained_variance   | 0.669      |\n",
    "    |    learning_rate        | 0.0003     |\n",
    "    |    loss                 | 54         |\n",
    "    |    n_updates            | 70         |\n",
    "    |    policy_gradient_loss | -0.015     |\n",
    "    |    std                  | 0.966      |\n",
    "    |    value_loss           | 84.9       |\n",
    "    ----------------------------------------\n",
    "\"\"\"\n",
    "time_steps = 500_000 # Adjust based on your training time\n",
    "ppo_model.learn(total_timesteps=time_steps)\n",
    "# progress_bar = tqdm(total=time_steps, desc=\"Training Progress\", unit=\"steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the trained model\n",
    "ppo_model.save(\"ppo_walker2d\") #If you want to continue training the model from where it left off, you can load the saved model and call learn() to continue the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 280.9908569574356, Std reward: 5.634012182536946\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Load the trained model and evaluate\n",
    "ppo_model = PPO.load(\"ppo_walker2d\")\n",
    "eval_env = gym.make(\"Walker2d-v5\", render_mode=\"human\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_env, n_eval_episodes=5)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "obs, _ = eval_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = ppo_model.predict(obs)\n",
    "    obs, reward, done, truncated, info = eval_env.step(action)\n",
    "    eval_env.render()\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
