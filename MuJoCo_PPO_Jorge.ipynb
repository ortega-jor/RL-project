{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Policy Network is correctly implemented!\n",
      "PPO custom_dump and custom_load methods work correctly!\n"
     ]
    }
   ],
   "source": [
    "class PPOPolicyNetwork(nn.Module): # Actor\n",
    "    def __init__(self, input_dim, output_dim, num_layers=3, hidden_dim=16):\n",
    "        super(PPOPolicyNetwork, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers # Ensure at least 2 layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))  # Output layer\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Proper weight initialization (important for stable training)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self): # for better convergence\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.network(x))  # Use tanh for continuous action spaces # Ensures actions remain in [-1,1]\n",
    "    \n",
    "    def custom_dump(self):\n",
    "        return {\n",
    "            'args': (self.input_dim, self.output_dim),\n",
    "            'kwargs': {\n",
    "                'num_layers': self.num_layers,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "            },\n",
    "            'state_dict': self.state_dict(),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def custom_load(cls, data):\n",
    "        model = cls(*data['args'], **data['kwargs'])\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "        return model\n",
    "    \n",
    "def _test_ppo_policy_forward(policy_model, input_shape, output_shape):\n",
    "    \"\"\"Tests that the PPO policy network returns correctly shaped tensors.\"\"\"\n",
    "    inputs = torch.randn(input_shape)  # Random input\n",
    "    outputs = policy_model(inputs)  # Forward pass\n",
    "\n",
    "    if not isinstance(outputs, torch.Tensor):\n",
    "        raise Exception(f'Policy forward returned {type(outputs)} instead of torch.Tensor')\n",
    "\n",
    "    if outputs.shape != output_shape:\n",
    "        raise Exception(f'Policy forward returned shape {outputs.shape}, expected {output_shape}')\n",
    "\n",
    "    if not outputs.requires_grad:\n",
    "        raise Exception('Policy forward output does not require a gradient (it should).')\n",
    "\n",
    "# Example Test\n",
    "obs_dim, act_dim = 17, 6  # Walker2d observation & action space\n",
    "policy_model = PPOPolicyNetwork(obs_dim, act_dim)\n",
    "_test_ppo_policy_forward(policy_model, (64, obs_dim), (64, act_dim))  # Batch size 64\n",
    "_test_ppo_policy_forward(policy_model, (10, obs_dim), (10, act_dim))  # Batch size 10\n",
    "print(\"PPO Policy Network is correctly implemented!\")\n",
    "\n",
    "# Testing custom dump / load\n",
    "ppo1 = PPOPolicyNetwork(17, 6, num_layers=4, hidden_dim=128)\n",
    "ppo_dump = ppo1.custom_dump()\n",
    "ppo2 = PPOPolicyNetwork.custom_load(ppo_dump)\n",
    "\n",
    "# Assertions to verify correct restoration\n",
    "assert ppo2.input_dim == 17\n",
    "assert ppo2.output_dim == 6\n",
    "assert ppo2.num_layers == 4\n",
    "assert ppo2.hidden_dim == 128\n",
    "print(\"PPO custom_dump and custom_load methods work correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Value Network is correctly implemented!\n",
      "PPOValueNetwork custom_dump and custom_load methods work correctly!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class PPOValueNetwork(nn.Module): # Critic\n",
    "    def __init__(self, input_dim, num_layers=3, hidden_dim=16):\n",
    "        super(PPOValueNetwork, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define layers similar to the PPOPolicyNetwork\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # Output layer (single scalar value)\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)  # Single value per input\n",
    "    \n",
    "        # Custom dump method for saving model state\n",
    "    def custom_dump(self):\n",
    "        return {\n",
    "            'args': (self.input_dim,),\n",
    "            'kwargs': {\n",
    "                'num_layers': self.num_layers,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "            },\n",
    "            'state_dict': self.state_dict(),\n",
    "        }\n",
    "\n",
    "    # Custom load method for restoring model state\n",
    "    @classmethod\n",
    "    def custom_load(cls, data):\n",
    "        model = cls(*data['args'], **data['kwargs'])\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "        return model    \n",
    "    \n",
    "def _test_ppo_value_forward(value_model, input_shape):\n",
    "    \"\"\"Tests that the PPO value network returns a correctly shaped tensor.\"\"\"\n",
    "    inputs = torch.randn(input_shape)  # Random input\n",
    "    outputs = value_model(inputs)  # Forward pass\n",
    "\n",
    "    if not isinstance(outputs, torch.Tensor):\n",
    "        raise Exception(f'Value forward returned {type(outputs)} instead of torch.Tensor')\n",
    "\n",
    "    if outputs.shape != (input_shape[0], 1):  # Expecting (batch_size, 1)\n",
    "        raise Exception(f'Value forward returned shape {outputs.shape}, expected ({input_shape[0]}, 1)')\n",
    "\n",
    "    if not outputs.requires_grad:\n",
    "        raise Exception('Value forward output does not require a gradient (it should).')\n",
    "\n",
    "# Example Test\n",
    "value_model = PPOValueNetwork(obs_dim)\n",
    "_test_ppo_value_forward(value_model, (64, obs_dim))  # Batch size 64\n",
    "_test_ppo_value_forward(value_model, (10, obs_dim))  # Batch size 10\n",
    "print(\"PPO Value Network is correctly implemented!\")\n",
    "\n",
    "# Testing custom dump / load\n",
    "value_model1 = PPOValueNetwork(obs_dim, num_layers=4, hidden_dim=128)\n",
    "value_dump = value_model1.custom_dump()\n",
    "value_model2 = PPOValueNetwork.custom_load(value_dump)\n",
    "\n",
    "# Assertions to verify correct restoration\n",
    "assert value_model2.input_dim == obs_dim\n",
    "assert value_model2.num_layers == 4\n",
    "assert value_model2.hidden_dim == 128\n",
    "print(\"PPOValueNetwork custom_dump and custom_load methods work correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, gamma=0.99, tau=0.95):\n",
    "    deltas = [] # Stores the advantages (deltas)\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * (values[t + 1] if t + 1 < len(values) else 0) - values[t] # Calculate the TD error (delta)\n",
    "        gae = delta + gamma * tau * gae # Compute the GAE (advantage)\n",
    "        deltas.insert(0, gae) # Insert the advantage at the front of the list (reverse order)\n",
    "    return deltas\n",
    "\n",
    "def ppo_loss(old_log_probs, new_log_probs, advantages, clip_epsilon):\n",
    "    # Compute the ratio (pi_theta / pi_theta_old)\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Compute the surrogate loss\n",
    "    obj_surrogate = ratio * advantages\n",
    "    obj_clipped = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "\n",
    "    # Final PPO objective (minimize the negative objective)\n",
    "    loss = -torch.min(obj_surrogate, obj_clipped).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, policy, value_net, policy_optimizer, value_optimizer, num_episodes=100, batch_size=64):\n",
    "    # Hyperparameters for PPO\n",
    "    gamma = 0.99\n",
    "    tau = 0.95\n",
    "    clip_epsilon = 0.2\n",
    "    n_epochs = 10  # Number of epochs to update the policy after each batch\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "        # Collect trajectory\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            action = policy(obs_tensor).detach().numpy()  # Get action from policy\n",
    "            log_prob = torch.log(policy(obs_tensor))  # Log probability of tak en action\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "        # Calculate values and advantages using the critic\n",
    "        values = value_net(states_tensor)\n",
    "        advantages = compute_gae(rewards_tensor, values, gamma, tau)\n",
    "\n",
    "        # Update policy and critic\n",
    "        for _ in range(n_epochs):\n",
    "            # Compute the loss for the policy\n",
    "            old_log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "            new_log_probs = torch.log(policy(states_tensor))\n",
    "\n",
    "            # Calculate PPO loss and perform backpropagation\n",
    "            policy_loss = ppo_loss(old_log_probs, new_log_probs, advantages, clip_epsilon)\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            # Compute the loss for the value function\n",
    "            value_loss = ((values - rewards_tensor) ** 2).mean()  # Mean squared error\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode} completed\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve this cell for all the modifying parameters functions\n",
    "def modify_mass_parameters(model):\n",
    "    # Modify the mass of specific body parts\n",
    "    model.body_mass[1] = 4.15  # Set mass of torso \n",
    "    model.body_mass[2], model.body_mass[5] = 0.6, 0.6  # Set mass of thigh\n",
    "    model.body_mass[3], model.body_mass[6] = 0.3, 0.3  # Set mass of leg\n",
    "    model.body_mass[4], model.body_mass[7]= 0.1, 0.1  # Set mass of foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9352\\3179395719.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states_tensor = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for dimension 0 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m value_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train PPO\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(env, policy, value_net, policy_optimizer, value_optimizer, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate values and advantages using the critic\u001b[39;00m\n\u001b[0;32m     33\u001b[0m values \u001b[38;5;241m=\u001b[39m value_net(states_tensor)\n\u001b[1;32m---> 34\u001b[0m advantages \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Update policy and critic\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Compute the loss for the policy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcompute_gae\u001b[1;34m(rewards, values, gamma, tau)\u001b[0m\n\u001b[0;32m      3\u001b[0m gae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rewards))):\n\u001b[1;32m----> 5\u001b[0m     delta \u001b[38;5;241m=\u001b[39m rewards[t] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m values[t]\n\u001b[0;32m      6\u001b[0m     gae \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m*\u001b[39m gae\n\u001b[0;32m      7\u001b[0m     deltas\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, gae)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 12 is out of bounds for dimension 0 with size 12"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = gym.make('Walker2d-v5')\n",
    "# env.reset()\n",
    "model = env.unwrapped.model\n",
    "modify_mass_parameters(model) # Modify mass parameters for the environment's model\n",
    "\n",
    "# Create policy and value networks\n",
    "policy = PPOPolicyNetwork(input_dim=env.observation_space.shape[0], output_dim=env.action_space.shape[0])\n",
    "value_net = PPOValueNetwork(input_dim=env.observation_space.shape[0])\n",
    "\n",
    "# Create optimizers\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=3e-4)\n",
    "\n",
    "# Train PPO\n",
    "train_ppo(env, policy, value_net, policy_optimizer, value_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
