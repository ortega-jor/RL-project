{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOPolicyNetwork(nn.Module): # Actor\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPOPolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        y = torch.tanh(self.fc3(x))  # Use tanh for continuous action spaces\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOValueNetwork(nn.Module): # Critic\n",
    "    def __init__(self, input_dim):\n",
    "        super(PPOValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)  # Output value (single scalar)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, gamma=0.99, tau=0.95):\n",
    "    deltas = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
    "        gae = delta + gamma * tau * gae\n",
    "        deltas.insert(0, gae)\n",
    "    return deltas\n",
    "\n",
    "def ppo_loss(old_log_probs, new_log_probs, advantages, clip_epsilon=0.2):\n",
    "    # Compute the ratio (pi_theta / pi_theta_old)\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Compute the surrogate loss\n",
    "    obj_surrogate = ratio * advantages\n",
    "    obj_clipped = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "\n",
    "    # Final PPO objective (minimize the negative objective)\n",
    "    loss = -torch.min(obj_surrogate, obj_clipped).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, policy, value_net, policy_optimizer, value_optimizer, num_episodes=100, batch_size=64):\n",
    "    # Hyperparameters for PPO\n",
    "    gamma = 0.99\n",
    "    tau = 0.95\n",
    "    clip_epsilon = 0.2\n",
    "    n_epochs = 10  # Number of epochs to update the policy after each batch\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "        # Collect trajectory\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "            action = policy(obs_tensor).detach().numpy()  # Get action from policy\n",
    "            log_prob = torch.log(policy(obs_tensor))  # Log probability of taken action\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "        # Calculate values and advantages using the critic\n",
    "        values = value_net(states_tensor)\n",
    "        advantages = compute_gae(rewards_tensor, values)\n",
    "\n",
    "        # Update policy and critic\n",
    "        for _ in range(n_epochs):\n",
    "            # Compute the loss for the policy\n",
    "            old_log_probs = torch.tensor(log_probs, dtype=torch.float32)\n",
    "            new_log_probs = torch.log(policy(states_tensor))\n",
    "\n",
    "            # Calculate PPO loss and perform backpropagation\n",
    "            policy_loss = ppo_loss(old_log_probs, new_log_probs, advantages)\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            # Compute the loss for the value function\n",
    "            value_loss = ((values - rewards_tensor) ** 2).mean()  # Mean squared error\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode} completed\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve this cell for all the modifying parameters functions\n",
    "def modify_mass_parameters(model):\n",
    "    # Modify the mass of specific body parts\n",
    "    model.body_mass[1] = 4.15  # Set mass of torso \n",
    "    model.body_mass[2], model.body_mass[5] = 0.6, 0.6  # Set mass of thigh\n",
    "    model.body_mass[3], model.body_mass[6] = 0.3, 0.3  # Set mass of leg\n",
    "    model.body_mass[4], model.body_mass[7]= 0.1, 0.1  # Set mass of foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_9352\\3179395719.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states_tensor = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for dimension 0 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m value_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train PPO\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(env, policy, value_net, policy_optimizer, value_optimizer, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate values and advantages using the critic\u001b[39;00m\n\u001b[0;32m     33\u001b[0m values \u001b[38;5;241m=\u001b[39m value_net(states_tensor)\n\u001b[1;32m---> 34\u001b[0m advantages \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Update policy and critic\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Compute the loss for the policy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcompute_gae\u001b[1;34m(rewards, values, gamma, tau)\u001b[0m\n\u001b[0;32m      3\u001b[0m gae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rewards))):\n\u001b[1;32m----> 5\u001b[0m     delta \u001b[38;5;241m=\u001b[39m rewards[t] \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m values[t]\n\u001b[0;32m      6\u001b[0m     gae \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m*\u001b[39m gae\n\u001b[0;32m      7\u001b[0m     deltas\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, gae)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 12 is out of bounds for dimension 0 with size 12"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = gym.make('Walker2d-v5')\n",
    "# env.reset()\n",
    "model = env.unwrapped.model\n",
    "modify_mass_parameters(model) # Modify mass parameters for the environment's model\n",
    "\n",
    "# Create policy and value networks\n",
    "policy = PPOPolicyNetwork(input_dim=env.observation_space.shape[0], output_dim=env.action_space.shape[0])\n",
    "value_net = PPOValueNetwork(input_dim=env.observation_space.shape[0])\n",
    "\n",
    "# Create optimizers\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=3e-4)\n",
    "\n",
    "# Train PPO\n",
    "train_ppo(env, policy, value_net, policy_optimizer, value_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
